dist: trusty
sudo: required
language: c
compiler: gcc

# This defines a "matrix" of jobs. Each combination of environment variables
# defines a different job. All of them run in parallel.
#
# FIXME: Each job starts with a cold Docker cache, which wastes work heating
# it up in parallel. It would be nice if "make test-build" could be done
# serially before splitting into parallel jobs.
#
#   EXPORT=         # build in Git checkout
#   EXPORT=yes      # build from "make export" tarball
#   INSTALL=        # run from build directory
#   INSTALL=yes     # make install to /usr/local, run that one
#   SETUID=         # standard build (user + mount namespaces)
#   SETUID=yes      # setuid build (mount namespace, setuid binary)
#
env:
  - EXPORT=    INSTALL=    SETUID=
  - EXPORT=    INSTALL=yes SETUID=
  - EXPORT=yes INSTALL=    SETUID=
  - EXPORT=yes INSTALL=yes SETUID=
  - EXPORT=    INSTALL=    SETUID=yes
  - EXPORT=    INSTALL=yes SETUID=yes
  - EXPORT=yes INSTALL=    SETUID=yes
  - EXPORT=yes INSTALL=yes SETUID=yes

install:
  - sudo apt-get install pigz
  - sudo pip install sphinx sphinx-rtd-theme

before_script:
  - getconf _NPROCESSORS_ONLN
  - free -m
  - df -h
  - df -h /var/tmp
  - export CH_TEST_TARDIR=/var/tmp/tarballs
  - export CH_TEST_IMGDIR=/var/tmp/images
  - export CH_TEST_PERMDIRS='/var/tmp /run'
  - export CH_TEST_OMIT=mpi
  - unset JAVA_HOME  # otherwise Spark tries to use host's Java
  - df -h
  - for d in $CH_TEST_PERMDIRS; do sudo test/make-perms-test $d $USER nobody; done

script:
  - test/travis.sh

after_script:
  - free -m
  - df -h
